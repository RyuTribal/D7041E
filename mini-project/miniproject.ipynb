{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a simple MLP using tutorial\n",
    "\n",
    "## Mini-project group 17: Andreas Holmstr√∂m and Ivan Sedelkin\n",
    "## Date: 2023-12-20\n",
    "\n",
    "This project is about implementing a simple Multilayer Perceptron. \n",
    "Here we try to train a network with different parameters. We have decided to experiment with different amount of hidden layers and different amount of hidden layer sizes. \n",
    "To make it easier to try different sizes, we decided to have the size of the next hidden layer to be half of the previous one for simplicity. This may affect performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from keras.datasets import mnist\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspiration:\n",
    "# https://machinelearningmastery.com/building-multilayer-perceptron-models-in-pytorch/\n",
    "# https://www.kaggle.com/code/riteshsinha/neural-networks-with-pytorch-mnist\n",
    "\n",
    "class SimpleMLP:\n",
    "    def __init__(self, training_data, training_labels, testing_data, testing_labels):\n",
    "        self.training_data = training_data\n",
    "        self.training_labels = training_labels\n",
    "        self.testing_data = testing_data\n",
    "        self.testing_labels = testing_labels\n",
    "        self.accuracy = 0\n",
    "\n",
    "    def create_model(self, model, number_of_layers, maximum_size_of_hidden_layer, input_size, output_size):\n",
    "        self.model = model\n",
    "        self.model.add_module(\"Input layer\",nn.Linear(input_size, maximum_size_of_hidden_layer))\n",
    "        self.model.add_module(\"Input layer activation\", nn.ReLU())\n",
    "        size = maximum_size_of_hidden_layer\n",
    "        for layer in range(0, number_of_layers):\n",
    "            self.model.add_module(\"Hidden layer number: \" + str((layer + 1)), nn.Linear(size, size//2))\n",
    "            self.model.add_module(\"Hidden layer activation number: \" + str((layer + 1)), nn.ReLU())\n",
    "            size = size//2\n",
    "        self.model.add_module(\"Output layer\", nn.Linear(size, output_size))\n",
    "\n",
    "    def train_model(self, loss_function, optimizer):\n",
    "        self.model.train(mode=True)\n",
    "        loss_fn = loss_function\n",
    "        optimizer = optimizer\n",
    "\n",
    "        for data, label in tqdm(zip(self.training_data, self.training_labels)):\n",
    "            y_pred = self.model(data)\n",
    "            loss = loss_fn(y_pred, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        self.model.train(mode=False)\n",
    "    \n",
    "    def test_model(self):\n",
    "        right_answers = 0\n",
    "        number_of_iterations = 0\n",
    "        for data, label in tqdm(zip(self.testing_data, self.testing_labels)):\n",
    "            y_pred = self.model(data)\n",
    "            _, predicted = torch.max(y_pred.data, 0)\n",
    "            if predicted.item() == label.item():\n",
    "                right_answers += 1\n",
    "            number_of_iterations += 1\n",
    "        \n",
    "        # Accuracy\n",
    "        accuracy = right_answers / number_of_iterations\n",
    "        self.accuracy = accuracy\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training data and test data from MNIST\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train_flattened = torch.tensor(x_train.reshape(60000, -1),dtype=torch.float32) # (60000, 784)\n",
    "y_train = torch.tensor(y_train) # (60000,)\n",
    "x_test_flattened = torch.tensor(x_test.reshape(10000, -1),dtype=torch.float32) # (10000, 784)\n",
    "y_test = torch.tensor(y_test) # (10000,)\n",
    "\n",
    "input_size = x_train_flattened.shape[1]\n",
    "output_size = 10\n",
    "number_of_hidden_layers = 1\n",
    "maximum_size_of_hidden_layer = 100\n",
    "\n",
    "training_data = x_train_flattened\n",
    "training_labels = y_train\n",
    "testing_data = x_test_flattened\n",
    "testing_labels = y_test\n",
    "\n",
    "hidden_layer_sizes = [100, 200, 400]\n",
    "hidden_layers = [1,2,4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy loss function using SGD as optimizer\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60000it [00:38, 1564.56it/s]\n",
      "10000it [00:01, 6530.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 1 hidden layers with 100 hidden layer maximum size was: \t90.23%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60000it [00:44, 1345.34it/s]\n",
      "10000it [00:01, 5004.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 2 hidden layers with 100 hidden layer maximum size was: \t94.18%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60000it [00:59, 1015.57it/s]\n",
      "10000it [00:02, 3539.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 4 hidden layers with 100 hidden layer maximum size was: \t92.42%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60000it [00:44, 1349.84it/s]\n",
      "10000it [00:01, 5726.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 1 hidden layers with 200 hidden layer maximum size was: \t89.56%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60000it [01:23, 715.10it/s]\n",
      "10000it [00:03, 3196.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 2 hidden layers with 200 hidden layer maximum size was: \t94.39%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60000it [01:39, 603.17it/s]\n",
      "10000it [00:04, 2170.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 4 hidden layers with 200 hidden layer maximum size was: \t93.84%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60000it [02:00, 498.76it/s]\n",
      "10000it [00:02, 3399.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 1 hidden layers with 400 hidden layer maximum size was: \t88.26%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60000it [02:04, 480.79it/s]\n",
      "10000it [00:02, 3728.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 2 hidden layers with 400 hidden layer maximum size was: \t95.11%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60000it [02:09, 463.15it/s]\n",
      "10000it [00:03, 2678.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 4 hidden layers with 400 hidden layer maximum size was: \t95.79%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------- Cross Entropy Loss ------------------------- #\n",
    "\n",
    "cost_function = nn.CrossEntropyLoss()\n",
    "print(\"Cross Entropy loss function using SGD as optimizer\\n\")\n",
    "# Adam was really slow.\n",
    "\n",
    "# Changing the amount of hidden layers and its maximum size\n",
    "for hidden_layer_size in hidden_layer_sizes:\n",
    "    for layers in hidden_layers:\n",
    "        mlp = SimpleMLP(training_data=training_data, training_labels=training_labels, testing_data=testing_data, testing_labels=testing_labels)\n",
    "        mlp.create_model(nn.Sequential(), layers, hidden_layer_size, input_size, output_size)\n",
    "        mlp.train_model(cost_function, torch.optim.SGD(mlp.model.parameters(), lr=0.001))\n",
    "        mlp.test_model()\n",
    "        print(f\"Accuracy for {str(layers)} hidden layers with {str(hidden_layer_size)} hidden layer maximum size was: \\t{mlp.accuracy*100:.2f}%\")\n",
    "        print()\n",
    "\n",
    "# ---------------------------------------------------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error loss function\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60000it [03:04, 325.24it/s]\n",
      "10000it [00:01, 5129.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 1 hidden layers with 100 hidden layer maximum size was: \t12.40%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60000it [03:46, 265.34it/s]\n",
      "10000it [00:03, 2926.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 2 hidden layers with 100 hidden layer maximum size was: \t6.65%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60000it [05:47, 172.87it/s]\n",
      "10000it [00:24, 404.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 4 hidden layers with 100 hidden layer maximum size was: \t2.79%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60000it [09:27, 105.82it/s]\n",
      "10000it [00:03, 3205.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 1 hidden layers with 200 hidden layer maximum size was: \t0.57%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60000it [08:08, 122.90it/s]\n",
      "10000it [00:04, 2314.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 2 hidden layers with 200 hidden layer maximum size was: \t0.44%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60000it [09:23, 106.42it/s]\n",
      "10000it [00:06, 1515.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 4 hidden layers with 200 hidden layer maximum size was: \t1.05%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60000it [10:56, 91.36it/s] \n",
      "10000it [00:03, 2864.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 1 hidden layers with 400 hidden layer maximum size was: \t11.49%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60000it [11:26, 87.35it/s] \n",
      "10000it [00:03, 3276.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 2 hidden layers with 400 hidden layer maximum size was: \t11.16%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60000it [14:15, 70.14it/s]\n",
      "10000it [00:06, 1513.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 4 hidden layers with 400 hidden layer maximum size was: \t13.89%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------- Mean Square Error -------------------------- #\n",
    "training_labels = training_labels.float()\n",
    "testing_labels = testing_labels.float()\n",
    "cost_function = nn.MSELoss()\n",
    "print(\"Mean Square Error loss function\\n\")\n",
    "# Had problems with SGD getting infinite gradients resulting in tensors of nan values and accuracies of 5-10%\n",
    "\n",
    "# Changing the amount of hidden layers and its maximum size\n",
    "for hidden_layer_size in hidden_layer_sizes:\n",
    "    for layers in hidden_layers:\n",
    "        mlp = SimpleMLP(training_data=training_data, training_labels=training_labels, testing_data=testing_data, testing_labels=testing_labels)\n",
    "        mlp.create_model(nn.Sequential(), layers, hidden_layer_size, input_size, output_size)\n",
    "        mlp.train_model(cost_function, torch.optim.Adam(mlp.model.parameters(), lr=0.001))\n",
    "        mlp.test_model()\n",
    "        print(f\"Accuracy for {str(layers)} hidden layers with {str(hidden_layer_size)} hidden layer maximum size was: \\t{mlp.accuracy*100:.2f}%\")\n",
    "        print()\n",
    "\n",
    "# ---------------------------------------------------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60000it [05:03, 197.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# ----------- BEST ONE FROM CROSS ENTROPY ----------- #\n",
    "training_labels = training_labels.long()\n",
    "\n",
    "mlp = SimpleMLP(training_data=training_data, training_labels=training_labels, testing_data=testing_data, testing_labels=testing_labels)\n",
    "mlp.create_model(nn.Sequential(), number_of_layers=4, maximum_size_of_hidden_layer=400, input_size=784, output_size=10)\n",
    "mlp.train_model(nn.CrossEntropyLoss(), torch.optim.SGD(mlp.model.parameters(), lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample selected: 1.0\n",
      "Prediction: tensor(1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------- DEMO --------------- #\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "rand_index = np.random.randint(0, testing_data.shape[0])\n",
    "random_sample_data = testing_data[rand_index]\n",
    "random_sample_label = testing_labels[rand_index]\n",
    "\n",
    "print(\"Random sample selected: \" + str(random_sample_label.item()))\n",
    "\n",
    "plt.imshow(random_sample_data.reshape(28,28))\n",
    "\n",
    "prediction = mlp.model(random_sample_data)\n",
    "_, prediction = torch.max(prediction, 0)\n",
    "print(\"Prediction: \" + str(prediction))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Training our model with the Entropy Loss function yielded really good results. \n",
    "From what we've read, this is the standard for multilabel classification. \n",
    "The issue seems to be trying other loss functions. \n",
    "We have tried both L1Loss, HingeEmbedding, however after more research Hinge seemed to be used for binary classification.\n",
    "\n",
    "From what we now MSE lossfunction is for regression problems, but looking at some different tutorials they seem to use MSE but didn't work for us. \n",
    "Therefore it is probably an implementation problem from our side. In this project we have tried different amount of hidden layers and hidden layer sizes. \n",
    "We did not implement each model by themselves, instead tried to do a more flexible testing class for trying different amount of hidden layers and hidden layer sizes, perhaps this is the reason for our MSE loss function not performing as it should.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
