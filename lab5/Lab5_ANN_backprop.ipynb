{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions 1\n",
    "\n",
    "#### A: Principle of backpropagation algorithm:\n",
    "Backpropagation is a process involved in training a neural network. It involves taking the error rate of a forward propagation and \n",
    "feeding this loss backward through the neural network layers to fine-tune the weights. \n",
    "#### B: The meaning and the role of the Softmax function:\n",
    "It converts the neural networks predictions into probability. \n",
    "\n",
    "#### C: Name typically used non-linear output functions and implications of choosing one or another for implementation:\n",
    "- **ReLu**: It is very efficient computationally\n",
    "- **Sigmoid**: It is used for probabalistic predictions because of it's range (between 0 and 1)\n",
    "- **SoftMax**: Similar to Sigmoid in that it gives a probability, but because of its summed nature it is usually used in the last layer since you usually cannot move forward with the result if it goes beyond 1\n",
    "- **Hyperbolic Tanget**: Very good for mapping outputs to states between \"negative\", \"neutral\" or \"positive\" and is solid for hidden layers as an activation function. Since tanh is zero-centered (meaning its outputs are centered around 0), it can help in reducing the bias shift effect during training. If the activations are not zero-centered, the gradients can consistently be all positive or all negative in certain layers, which can lead to inefficient gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions of non-linear activations\n",
    "def f_sigmoid(X, deriv=False):\n",
    "    if not deriv:\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    else:\n",
    "        return f_sigmoid(X)*(1 - f_sigmoid(X))\n",
    "\n",
    "\n",
    "def f_softmax(X):\n",
    "    Z = np.sum(np.exp(X), axis=1)\n",
    "    Z = Z.reshape(Z.shape[0], 1)\n",
    "    return np.exp(X) / Z\n",
    "\n",
    "def f_relu(X, deriv=False):\n",
    "    if not deriv:\n",
    "        return np.maximum(0, X)\n",
    "    else:\n",
    "        return (X > 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exit_with_err(err_str):\n",
    "    print >> sys.stderr, err_str\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functionality of a single hidden layer\n",
    "class Layer:\n",
    "    def __init__(self, size, batch_size, is_input=False, is_output=False,\n",
    "                 activation=f_sigmoid):\n",
    "        self.is_input = is_input\n",
    "        self.is_output = is_output\n",
    "\n",
    "        # Z is the matrix that holds output values\n",
    "        self.Z = np.zeros((batch_size, size[0]))\n",
    "        # The activation function is an externally defined function (with a\n",
    "        # derivative) that is stored here\n",
    "        self.activation = activation\n",
    "\n",
    "        # W is the outgoing weight matrix for this layer\n",
    "        self.W = None\n",
    "        # S is the matrix that holds the inputs to this layer\n",
    "        self.S = None\n",
    "        # D is the matrix that holds the deltas for this layer\n",
    "        self.D = None\n",
    "        # Fp is the matrix that holds the derivatives of the activation function\n",
    "        self.Fp = None\n",
    "\n",
    "        if not is_input:\n",
    "            self.S = np.zeros((batch_size, size[0]))\n",
    "            self.D = np.zeros((batch_size, size[0]))\n",
    "\n",
    "        if not is_output:\n",
    "            self.W = np.random.normal(size=size, scale=1E-4)\n",
    "\n",
    "        if not is_input and not is_output:\n",
    "            self.Fp = np.zeros((size[0], batch_size))\n",
    "\n",
    "    def forward_propagate(self):\n",
    "        if self.is_input:\n",
    "            return self.Z.dot(self.W)\n",
    "\n",
    "        self.Z = self.activation(self.S)\n",
    "        if self.is_output:\n",
    "            return self.Z\n",
    "        else:\n",
    "            # For hidden layers, we add the bias values here\n",
    "            self.Z = np.append(self.Z, np.ones((self.Z.shape[0], 1)), axis=1)\n",
    "            self.Fp = self.activation(self.S, deriv=True).T\n",
    "            return self.Z.dot(self.W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, layer_config, batch_size=100):\n",
    "        self.layers = []\n",
    "        self.num_layers = len(layer_config)\n",
    "        self.minibatch_size = batch_size\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            if i == 0:\n",
    "                print (\"Initializing input layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here, we add an additional unit at the input for the bias\n",
    "                # weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         is_input=True))\n",
    "            else:\n",
    "                print (\"Initializing hidden layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here we add an additional unit in the hidden layers for the\n",
    "                # bias weight.\n",
    "                # self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                #                          batch_size,\n",
    "                #                          activation=f_sigmoid))\n",
    "                \n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         activation=f_relu))\n",
    "\n",
    "        print (\"Initializing output layer with size {0}.\".format(layer_config[-1]))\n",
    "        self.layers.append(Layer([layer_config[-1], None],\n",
    "                                 batch_size,\n",
    "                                 is_output=True,\n",
    "                                 activation=f_softmax))\n",
    "        print (\"Done!\")\n",
    "\n",
    "    def forward_propagate(self, data):\n",
    "        # We need to be sure to add bias values to the input\n",
    "        self.layers[0].Z = np.append(data, np.ones((data.shape[0], 1)), axis=1)\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.layers[i+1].S = self.layers[i].forward_propagate()\n",
    "        return self.layers[-1].forward_propagate()\n",
    "\n",
    "    def backpropagate(self, yhat, labels):\n",
    "        \n",
    "        # exit_with_err(\"FIND ME IN THE CODE, What is computed in the next line of code?\\n\")\n",
    "\n",
    "        # It calculates the initial gradient of the loss function \n",
    "        # with respect to the output of the last layer (the output predictions) \n",
    "        # and stores it in the last layer's D property.\n",
    "\n",
    "        self.layers[-1].D = (yhat - labels).T\n",
    "        for i in range(self.num_layers-2, 0, -1):\n",
    "            # We do not calculate deltas for the bias values\n",
    "            W_nobias = self.layers[i].W[0:-1, :]\n",
    "\n",
    "            # exit_with_err(\"FIND ME IN THE CODE, What does this 'for' loop do?\\n\")\n",
    "\n",
    "            # It goes through the network of layers from the back and updates the deltas for the hidden layers.            \n",
    "            \n",
    "            self.layers[i].D = W_nobias.dot(self.layers[i+1].D) * self.layers[i].Fp\n",
    "\n",
    "    def update_weights(self, eta):\n",
    "        for i in range(0, self.num_layers-1):\n",
    "            W_grad = -eta*(self.layers[i+1].D.dot(self.layers[i].Z)).T\n",
    "            self.layers[i].W += W_grad\n",
    "\n",
    "    def evaluate(self, train_data, train_labels, test_data, test_labels,\n",
    "                 num_epochs=70, eta=0.05, eval_train=False, eval_test=True):\n",
    "\n",
    "        N_train = len(train_labels)*len(train_labels[0])\n",
    "        N_test = len(test_labels)*len(test_labels[0])\n",
    "\n",
    "        print (\"Training for {0} epochs...\".format(num_epochs))\n",
    "        for t in range(0, num_epochs):\n",
    "            out_str = \"[{0:4d}] \".format(t)\n",
    "\n",
    "            for b_data, b_labels in zip(train_data, train_labels):\n",
    "                output = self.forward_propagate(b_data)\n",
    "                self.backpropagate(output, b_labels)\n",
    "                \n",
    "                # exit_with_err(\"FIND ME IN THE CODE, How does weight update is implemented? What is eta?\\n\")\n",
    "\n",
    "                # ETA is the learning rate.\n",
    "                # The update function is implemented by multiplying the gradient by the learning rate and subtracting it from the weights.\n",
    "\n",
    "                self.update_weights(eta=eta)\n",
    "\n",
    "            if eval_train:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(train_data, train_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Training error: {1:.5f}\".format(out_str,\n",
    "                                                           float(errs)/N_train))\n",
    "\n",
    "            if eval_test:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(test_data, test_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Test error: {1:.5f}\").format(out_str,\n",
    "                                                       float(errs)/N_test)\n",
    "\n",
    "            print (out_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_bit_vector(labels, nbits):\n",
    "    bit_vector = np.zeros((labels.shape[0], nbits))\n",
    "    for i in range(labels.shape[0]):\n",
    "        bit_vector[i, labels[i]] = 1.0\n",
    "\n",
    "    return bit_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(data, labels, batch_size, create_bit_vector=False):\n",
    "    N = data.shape[0]\n",
    "    print (\"Batch size {0}, the number of examples {1}.\".format(batch_size,N))\n",
    "\n",
    "    if N % batch_size != 0:\n",
    "        print (\"Warning in create_minibatches(): Batch size {0} does not \" \\\n",
    "              \"evenly divide the number of examples {1}.\".format(batch_size,N))\n",
    "    chunked_data = []\n",
    "    chunked_labels = []\n",
    "    idx = 0\n",
    "    while idx + batch_size <= N:\n",
    "        chunked_data.append(data[idx:idx+batch_size, :])\n",
    "        if not create_bit_vector:\n",
    "            chunked_labels.append(labels[idx:idx+batch_size])\n",
    "        else:\n",
    "            bit_vector = label_to_bit_vector(labels[idx:idx+batch_size], 10)\n",
    "            chunked_labels.append(bit_vector)\n",
    "\n",
    "        idx += batch_size\n",
    "\n",
    "    return chunked_data, chunked_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_backprop(batch_size, Train_images, Train_labels, Valid_images, Valid_labels):\n",
    "    \n",
    "    print (\"Creating data...\")\n",
    "    batched_train_data, batched_train_labels = create_batches(Train_images, Train_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    batched_valid_data, batched_valid_labels = create_batches(Valid_images, Valid_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    print (\"Done!\")\n",
    "\n",
    "\n",
    "    return batched_train_data, batched_train_labels,  batched_valid_data, batched_valid_labels\n",
    "\n",
    "def get_accuracy(model, X, y):\n",
    "    yhat = model.forward_propagate(X)\n",
    "    yhat = np.argmax(yhat, axis=1)\n",
    "    accuracy = np.sum(yhat == y) / float(len(y))\n",
    "    print(\"Accuracy: {0:.4f}\".format(accuracy))\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(Xtr, Ltr), (X_test, L_test)=mnist.load_data()\n",
    "\n",
    "Xtr = Xtr.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "Xtr = Xtr.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "Xtr /= 255\n",
    "X_test /= 255\n",
    "print(Xtr.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0]  Training error: 0.46972 Test error: 0.47070\n",
      "[   1]  Training error: 0.07760 Test error: 0.07590\n",
      "[   2]  Training error: 0.04945 Test error: 0.05290\n",
      "[   3]  Training error: 0.04372 Test error: 0.04740\n",
      "[   4]  Training error: 0.03355 Test error: 0.03800\n",
      "[   5]  Training error: 0.03038 Test error: 0.03970\n",
      "[   6]  Training error: 0.02590 Test error: 0.03690\n",
      "[   7]  Training error: 0.02168 Test error: 0.03310\n",
      "[   8]  Training error: 0.02067 Test error: 0.03440\n",
      "[   9]  Training error: 0.02007 Test error: 0.03360\n",
      "[  10]  Training error: 0.02150 Test error: 0.03700\n",
      "[  11]  Training error: 0.01632 Test error: 0.03440\n",
      "[  12]  Training error: 0.01758 Test error: 0.03280\n",
      "[  13]  Training error: 0.02112 Test error: 0.03510\n",
      "[  14]  Training error: 0.01668 Test error: 0.03280\n",
      "[  15]  Training error: 0.01080 Test error: 0.03160\n",
      "[  16]  Training error: 0.01162 Test error: 0.03210\n",
      "[  17]  Training error: 0.01190 Test error: 0.03230\n",
      "[  18]  Training error: 0.01195 Test error: 0.03170\n",
      "[  19]  Training error: 0.01633 Test error: 0.03440\n",
      "[  20]  Training error: 0.00995 Test error: 0.03040\n",
      "[  21]  Training error: 0.00918 Test error: 0.03190\n",
      "[  22]  Training error: 0.01158 Test error: 0.03160\n",
      "[  23]  Training error: 0.01117 Test error: 0.03230\n",
      "[  24]  Training error: 0.00945 Test error: 0.03130\n",
      "[  25]  Training error: 0.00598 Test error: 0.02940\n",
      "[  26]  Training error: 0.00977 Test error: 0.03100\n",
      "[  27]  Training error: 0.00615 Test error: 0.02880\n",
      "[  28]  Training error: 0.00538 Test error: 0.02850\n",
      "[  29]  Training error: 0.00877 Test error: 0.03090\n",
      "[  30]  Training error: 0.00315 Test error: 0.02880\n",
      "[  31]  Training error: 0.00598 Test error: 0.03130\n",
      "[  32]  Training error: 0.00407 Test error: 0.02820\n",
      "[  33]  Training error: 0.00422 Test error: 0.02910\n",
      "[  34]  Training error: 0.00788 Test error: 0.02870\n",
      "[  35]  Training error: 0.00722 Test error: 0.03100\n",
      "[  36]  Training error: 0.00388 Test error: 0.02710\n",
      "[  37]  Training error: 0.00672 Test error: 0.02970\n",
      "[  38]  Training error: 0.00623 Test error: 0.03030\n",
      "[  39]  Training error: 0.01087 Test error: 0.03440\n",
      "[  40]  Training error: 0.00490 Test error: 0.02880\n",
      "[  41]  Training error: 0.00515 Test error: 0.03040\n",
      "[  42]  Training error: 0.00547 Test error: 0.02980\n",
      "[  43]  Training error: 0.00242 Test error: 0.02730\n",
      "[  44]  Training error: 0.00215 Test error: 0.02850\n",
      "[  45]  Training error: 0.00113 Test error: 0.02670\n",
      "[  46]  Training error: 0.00093 Test error: 0.02650\n",
      "[  47]  Training error: 0.00033 Test error: 0.02540\n",
      "[  48]  Training error: 0.00018 Test error: 0.02600\n",
      "[  49]  Training error: 0.00007 Test error: 0.02490\n",
      "[  50]  Training error: 0.00005 Test error: 0.02530\n",
      "[  51]  Training error: 0.00005 Test error: 0.02520\n",
      "[  52]  Training error: 0.00005 Test error: 0.02560\n",
      "[  53]  Training error: 0.00005 Test error: 0.02560\n",
      "[  54]  Training error: 0.00003 Test error: 0.02540\n",
      "[  55]  Training error: 0.00002 Test error: 0.02510\n",
      "[  56]  Training error: 0.00002 Test error: 0.02520\n",
      "[  57]  Training error: 0.00002 Test error: 0.02490\n",
      "[  58]  Training error: 0.00002 Test error: 0.02490\n",
      "[  59]  Training error: 0.00002 Test error: 0.02490\n",
      "[  60]  Training error: 0.00000 Test error: 0.02490\n",
      "[  61]  Training error: 0.00000 Test error: 0.02480\n",
      "[  62]  Training error: 0.00000 Test error: 0.02480\n",
      "[  63]  Training error: 0.00000 Test error: 0.02480\n",
      "[  64]  Training error: 0.00000 Test error: 0.02500\n",
      "[  65]  Training error: 0.00000 Test error: 0.02490\n",
      "[  66]  Training error: 0.00000 Test error: 0.02510\n",
      "[  67]  Training error: 0.00000 Test error: 0.02490\n",
      "[  68]  Training error: 0.00000 Test error: 0.02490\n",
      "[  69]  Training error: 0.00000 Test error: 0.02480\n",
      "Accuracy: 0.9752\n",
      "Done:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,\n",
    "             eval_train=True)\n",
    "\n",
    "get_accuracy(mlp, X_test, L_test)\n",
    "\n",
    "print(\"Done:)\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n",
      "[   0]  Training error: 0.89782 Test error: 0.89900\n",
      "[   1]  Training error: 0.90085 Test error: 0.89910\n",
      "[   2]  Training error: 0.90128 Test error: 0.90200\n",
      "[   3]  Training error: 0.90085 Test error: 0.89910\n",
      "[   4]  Training error: 0.90137 Test error: 0.90420\n",
      "[   5]  Training error: 0.90128 Test error: 0.90200\n",
      "[   6]  Training error: 0.88763 Test error: 0.88650\n",
      "[   7]  Training error: 0.90137 Test error: 0.90420\n",
      "[   8]  Training error: 0.89782 Test error: 0.89900\n",
      "[   9]  Training error: 0.90263 Test error: 0.90180\n",
      "[  10]  Training error: 0.90085 Test error: 0.89910\n",
      "[  11]  Training error: 0.89782 Test error: 0.89900\n",
      "[  12]  Training error: 0.89782 Test error: 0.89900\n",
      "[  13]  Training error: 0.90137 Test error: 0.90420\n",
      "[  14]  Training error: 0.90128 Test error: 0.90200\n",
      "[  15]  Training error: 0.90128 Test error: 0.90200\n",
      "[  16]  Training error: 0.90263 Test error: 0.90180\n",
      "[  17]  Training error: 0.90965 Test error: 0.91080\n",
      "[  18]  Training error: 0.90263 Test error: 0.90180\n",
      "[  19]  Training error: 0.89558 Test error: 0.89720\n",
      "[  20]  Training error: 0.90128 Test error: 0.90200\n",
      "[  21]  Training error: 0.90248 Test error: 0.90260\n",
      "[  22]  Training error: 0.90263 Test error: 0.90180\n",
      "[  23]  Training error: 0.89782 Test error: 0.89900\n",
      "[  24]  Training error: 0.90128 Test error: 0.90200\n",
      "[  25]  Training error: 0.90248 Test error: 0.90260\n",
      "[  26]  Training error: 0.90248 Test error: 0.90260\n",
      "[  27]  Training error: 0.90965 Test error: 0.91080\n",
      "[  28]  Training error: 0.90085 Test error: 0.89910\n",
      "[  29]  Training error: 0.90965 Test error: 0.91080\n",
      "[  30]  Training error: 0.89558 Test error: 0.89720\n",
      "[  31]  Training error: 0.90137 Test error: 0.90420\n",
      "[  32]  Training error: 0.90137 Test error: 0.90420\n",
      "[  33]  Training error: 0.90248 Test error: 0.90260\n",
      "[  34]  Training error: 0.90248 Test error: 0.90260\n",
      "[  35]  Training error: 0.90248 Test error: 0.90260\n",
      "[  36]  Training error: 0.90128 Test error: 0.90200\n",
      "[  37]  Training error: 0.90248 Test error: 0.90260\n",
      "[  38]  Training error: 0.90263 Test error: 0.90180\n",
      "[  39]  Training error: 0.89782 Test error: 0.89900\n",
      "[  40]  Training error: 0.90263 Test error: 0.90180\n",
      "[  41]  Training error: 0.89782 Test error: 0.89900\n",
      "[  42]  Training error: 0.90085 Test error: 0.89910\n",
      "[  43]  Training error: 0.90085 Test error: 0.89910\n",
      "[  44]  Training error: 0.90128 Test error: 0.90200\n",
      "[  45]  Training error: 0.88763 Test error: 0.88650\n",
      "[  46]  Training error: 0.90263 Test error: 0.90180\n",
      "[  47]  Training error: 0.90248 Test error: 0.90260\n",
      "[  48]  Training error: 0.90248 Test error: 0.90260\n",
      "[  49]  Training error: 0.90248 Test error: 0.90260\n",
      "[  50]  Training error: 0.90137 Test error: 0.90420\n",
      "[  51]  Training error: 0.90070 Test error: 0.89680\n",
      "[  52]  Training error: 0.90248 Test error: 0.90260\n",
      "[  53]  Training error: 0.90248 Test error: 0.90260\n",
      "[  54]  Training error: 0.88763 Test error: 0.88650\n",
      "[  55]  Training error: 0.88763 Test error: 0.88650\n",
      "[  56]  Training error: 0.89558 Test error: 0.89720\n",
      "[  57]  Training error: 0.90248 Test error: 0.90260\n",
      "[  58]  Training error: 0.90965 Test error: 0.91080\n",
      "[  59]  Training error: 0.90263 Test error: 0.90180\n",
      "[  60]  Training error: 0.89782 Test error: 0.89900\n",
      "[  61]  Training error: 0.88763 Test error: 0.88650\n",
      "[  62]  Training error: 0.90263 Test error: 0.90180\n",
      "[  63]  Training error: 0.90085 Test error: 0.89910\n",
      "[  64]  Training error: 0.90137 Test error: 0.90420\n",
      "[  65]  Training error: 0.90248 Test error: 0.90260\n",
      "[  66]  Training error: 0.90085 Test error: 0.89910\n",
      "[  67]  Training error: 0.90137 Test error: 0.90420\n",
      "[  68]  Training error: 0.90137 Test error: 0.90420\n",
      "[  69]  Training error: 0.90248 Test error: 0.90260\n",
      "Accuracy: 0.0974\n",
      "Done:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,\n",
    "             eval_train=True, eta=0.5)\n",
    "\n",
    "get_accuracy(mlp, X_test, L_test)\n",
    "\n",
    "print(\"Done:)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n",
      "[   0]  Training error: 0.70343 Test error: 0.70080\n",
      "[   1]  Training error: 0.64710 Test error: 0.64380\n",
      "[   2]  Training error: 0.59973 Test error: 0.59910\n",
      "[   3]  Training error: 0.45603 Test error: 0.46660\n",
      "[   4]  Training error: 0.20322 Test error: 0.19170\n",
      "[   5]  Training error: 0.11390 Test error: 0.11040\n",
      "[   6]  Training error: 0.09023 Test error: 0.08900\n",
      "[   7]  Training error: 0.07532 Test error: 0.07420\n",
      "[   8]  Training error: 0.06420 Test error: 0.06520\n",
      "[   9]  Training error: 0.05513 Test error: 0.05750\n",
      "[  10]  Training error: 0.04828 Test error: 0.05070\n",
      "[  11]  Training error: 0.04290 Test error: 0.04760\n",
      "[  12]  Training error: 0.03870 Test error: 0.04340\n",
      "[  13]  Training error: 0.03508 Test error: 0.04050\n",
      "[  14]  Training error: 0.03197 Test error: 0.03690\n",
      "[  15]  Training error: 0.02883 Test error: 0.03520\n",
      "[  16]  Training error: 0.02600 Test error: 0.03400\n",
      "[  17]  Training error: 0.02405 Test error: 0.03220\n",
      "[  18]  Training error: 0.02220 Test error: 0.03120\n",
      "[  19]  Training error: 0.02065 Test error: 0.03080\n",
      "[  20]  Training error: 0.01920 Test error: 0.03020\n",
      "[  21]  Training error: 0.01782 Test error: 0.02920\n",
      "[  22]  Training error: 0.01678 Test error: 0.02790\n",
      "[  23]  Training error: 0.01555 Test error: 0.02800\n",
      "[  24]  Training error: 0.01435 Test error: 0.02820\n",
      "[  25]  Training error: 0.01355 Test error: 0.02790\n",
      "[  26]  Training error: 0.01268 Test error: 0.02770\n",
      "[  27]  Training error: 0.01195 Test error: 0.02730\n",
      "[  28]  Training error: 0.01125 Test error: 0.02680\n",
      "[  29]  Training error: 0.01063 Test error: 0.02680\n",
      "[  30]  Training error: 0.00990 Test error: 0.02710\n",
      "[  31]  Training error: 0.00955 Test error: 0.02670\n",
      "[  32]  Training error: 0.00915 Test error: 0.02700\n",
      "[  33]  Training error: 0.00860 Test error: 0.02730\n",
      "[  34]  Training error: 0.00822 Test error: 0.02740\n",
      "[  35]  Training error: 0.00793 Test error: 0.02750\n",
      "[  36]  Training error: 0.00740 Test error: 0.02760\n",
      "[  37]  Training error: 0.00687 Test error: 0.02760\n",
      "[  38]  Training error: 0.00653 Test error: 0.02790\n",
      "[  39]  Training error: 0.00612 Test error: 0.02780\n",
      "[  40]  Training error: 0.00570 Test error: 0.02780\n",
      "[  41]  Training error: 0.00530 Test error: 0.02810\n",
      "[  42]  Training error: 0.00483 Test error: 0.02830\n",
      "[  43]  Training error: 0.00447 Test error: 0.02830\n",
      "[  44]  Training error: 0.00388 Test error: 0.02780\n",
      "[  45]  Training error: 0.00368 Test error: 0.02780\n",
      "[  46]  Training error: 0.00340 Test error: 0.02800\n",
      "[  47]  Training error: 0.00310 Test error: 0.02760\n",
      "[  48]  Training error: 0.00285 Test error: 0.02760\n",
      "[  49]  Training error: 0.00255 Test error: 0.02710\n",
      "[  50]  Training error: 0.00228 Test error: 0.02740\n",
      "[  51]  Training error: 0.00208 Test error: 0.02720\n",
      "[  52]  Training error: 0.00185 Test error: 0.02740\n",
      "[  53]  Training error: 0.00168 Test error: 0.02740\n",
      "[  54]  Training error: 0.00148 Test error: 0.02780\n",
      "[  55]  Training error: 0.00132 Test error: 0.02770\n",
      "[  56]  Training error: 0.00127 Test error: 0.02740\n",
      "[  57]  Training error: 0.00113 Test error: 0.02760\n",
      "[  58]  Training error: 0.00108 Test error: 0.02750\n",
      "[  59]  Training error: 0.00100 Test error: 0.02700\n",
      "[  60]  Training error: 0.00093 Test error: 0.02700\n",
      "[  61]  Training error: 0.00083 Test error: 0.02710\n",
      "[  62]  Training error: 0.00077 Test error: 0.02720\n",
      "[  63]  Training error: 0.00073 Test error: 0.02730\n",
      "[  64]  Training error: 0.00060 Test error: 0.02740\n",
      "[  65]  Training error: 0.00055 Test error: 0.02730\n",
      "[  66]  Training error: 0.00052 Test error: 0.02730\n",
      "[  67]  Training error: 0.00045 Test error: 0.02720\n",
      "[  68]  Training error: 0.00042 Test error: 0.02720\n",
      "[  69]  Training error: 0.00040 Test error: 0.02710\n",
      "Accuracy: 0.9729\n",
      "Done:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,\n",
    "             eval_train=True, eta=0.005)\n",
    "\n",
    "get_accuracy(mlp, X_test, L_test)\n",
    "\n",
    "print(\"Done:)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With ReLu\n",
      "\n",
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n",
      "[   0]  Training error: 0.90137 Test error: 0.90420\n",
      "[   1]  Training error: 0.90137 Test error: 0.90420\n",
      "[   2]  Training error: 0.90137 Test error: 0.90420\n",
      "[   3]  Training error: 0.90137 Test error: 0.90420\n",
      "[   4]  Training error: 0.90137 Test error: 0.90420\n",
      "[   5]  Training error: 0.90137 Test error: 0.90420\n",
      "[   6]  Training error: 0.90137 Test error: 0.90420\n",
      "[   7]  Training error: 0.90137 Test error: 0.90420\n",
      "[   8]  Training error: 0.90137 Test error: 0.90420\n",
      "[   9]  Training error: 0.90137 Test error: 0.90420\n",
      "[  10]  Training error: 0.90137 Test error: 0.90420\n",
      "[  11]  Training error: 0.90137 Test error: 0.90420\n",
      "[  12]  Training error: 0.90137 Test error: 0.90420\n",
      "[  13]  Training error: 0.90137 Test error: 0.90420\n",
      "[  14]  Training error: 0.90137 Test error: 0.90420\n",
      "[  15]  Training error: 0.90137 Test error: 0.90420\n",
      "[  16]  Training error: 0.90137 Test error: 0.90420\n",
      "[  17]  Training error: 0.90137 Test error: 0.90420\n",
      "[  18]  Training error: 0.90137 Test error: 0.90420\n",
      "[  19]  Training error: 0.90137 Test error: 0.90420\n",
      "[  20]  Training error: 0.90137 Test error: 0.90420\n",
      "[  21]  Training error: 0.90137 Test error: 0.90420\n",
      "[  22]  Training error: 0.90137 Test error: 0.90420\n",
      "[  23]  Training error: 0.90137 Test error: 0.90420\n",
      "[  24]  Training error: 0.90137 Test error: 0.90420\n",
      "[  25]  Training error: 0.90137 Test error: 0.90420\n",
      "[  26]  Training error: 0.90137 Test error: 0.90420\n",
      "[  27]  Training error: 0.90137 Test error: 0.90420\n",
      "[  28]  Training error: 0.90137 Test error: 0.90420\n",
      "[  29]  Training error: 0.90137 Test error: 0.90420\n",
      "[  30]  Training error: 0.90137 Test error: 0.90420\n",
      "[  31]  Training error: 0.90137 Test error: 0.90420\n",
      "[  32]  Training error: 0.90137 Test error: 0.90420\n",
      "[  33]  Training error: 0.90137 Test error: 0.90420\n",
      "[  34]  Training error: 0.90137 Test error: 0.90420\n",
      "[  35]  Training error: 0.90137 Test error: 0.90420\n",
      "[  36]  Training error: 0.90137 Test error: 0.90420\n",
      "[  37]  Training error: 0.90137 Test error: 0.90420\n",
      "[  38]  Training error: 0.90137 Test error: 0.90420\n",
      "[  39]  Training error: 0.90137 Test error: 0.90420\n",
      "[  40]  Training error: 0.90137 Test error: 0.90420\n",
      "[  41]  Training error: 0.90137 Test error: 0.90420\n",
      "[  42]  Training error: 0.90137 Test error: 0.90420\n",
      "[  43]  Training error: 0.90137 Test error: 0.90420\n",
      "[  44]  Training error: 0.90137 Test error: 0.90420\n",
      "[  45]  Training error: 0.90137 Test error: 0.90420\n",
      "[  46]  Training error: 0.90137 Test error: 0.90420\n",
      "[  47]  Training error: 0.90137 Test error: 0.90420\n",
      "[  48]  Training error: 0.90137 Test error: 0.90420\n",
      "[  49]  Training error: 0.90137 Test error: 0.90420\n",
      "[  50]  Training error: 0.90137 Test error: 0.90420\n",
      "[  51]  Training error: 0.90137 Test error: 0.90420\n",
      "[  52]  Training error: 0.90137 Test error: 0.90420\n",
      "[  53]  Training error: 0.90137 Test error: 0.90420\n",
      "[  54]  Training error: 0.90137 Test error: 0.90420\n",
      "[  55]  Training error: 0.90137 Test error: 0.90420\n",
      "[  56]  Training error: 0.90137 Test error: 0.90420\n",
      "[  57]  Training error: 0.90137 Test error: 0.90420\n",
      "[  58]  Training error: 0.90137 Test error: 0.90420\n",
      "[  59]  Training error: 0.90137 Test error: 0.90420\n",
      "[  60]  Training error: 0.90137 Test error: 0.90420\n",
      "[  61]  Training error: 0.90137 Test error: 0.90420\n",
      "[  62]  Training error: 0.90137 Test error: 0.90420\n",
      "[  63]  Training error: 0.90137 Test error: 0.90420\n",
      "[  64]  Training error: 0.90137 Test error: 0.90420\n",
      "[  65]  Training error: 0.90137 Test error: 0.90420\n",
      "[  66]  Training error: 0.90137 Test error: 0.90420\n",
      "[  67]  Training error: 0.90137 Test error: 0.90420\n",
      "[  68]  Training error: 0.90137 Test error: 0.90420\n",
      "[  69]  Training error: 0.90137 Test error: 0.90420\n",
      "Accuracy: 0.0958\n",
      "Done:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "\n",
    "print(\"With ReLu\\n\")\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,\n",
    "             eval_train=True)\n",
    "\n",
    "get_accuracy(mlp, X_test, L_test)\n",
    "\n",
    "print(\"Done:)\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
